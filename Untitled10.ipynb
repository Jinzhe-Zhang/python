{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第1页抓取完毕\n",
      "第2页抓取完毕\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import re\n",
    "from requests.exceptions import RequestException\n",
    "from bs4 import BeautifulSoup\n",
    "import csv\n",
    "import time\n",
    "\n",
    "headers = {'User-Agent':'Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/55.0.2883.87 Safari/537.36'}\n",
    "\n",
    "def get_one_page(url):\n",
    "    try:\n",
    "        response = requests.get(url,headers = headers)\n",
    "        if response.status_code == 200:\n",
    "            return response.text\n",
    "        return None\n",
    "    except RequestException:\n",
    "        return None\n",
    "\n",
    "def parse_one_page(content):\n",
    "    try:\n",
    "        soup = BeautifulSoup(content,'html.parser')\n",
    "        items = soup.find('div',class_=re.compile('js-tips-list'))\n",
    "        for div in items.find_all('div',class_=re.compile('ershoufang-list')):\n",
    "            yield {\n",
    "                #'Name':div.find('a',class_=re.compile('js-title')).text,\n",
    "                #'Type': div.find('dd', class_=re.compile('size')).contents[1].text,#tag的 .contents 属性可以将tag的子节点以列表的方式输出\n",
    "                #'Area':div.find('dd',class_=re.compile('size')).contents[5].text,\n",
    "                #'Towards':div.find('dd',class_=re.compile('size')).contents[9].text,\n",
    "                #'Floor':div.find('dd',class_=re.compile('size')).contents[13].text.replace('\\n',''),\n",
    "                #'Decorate':div.find('dd',class_=re.compile('size')).contents[17].text,\n",
    "                'Address':div.find('span',class_=re.compile('address')).text#,\n",
    "                #'TotalPrice':div.find('span',class_=re.compile('js-price')).text+div.find('span',class_=re.compile('yue')).text,\n",
    "                #'Price':div.find('div',class_=re.compile('time')).text\n",
    "            }\n",
    "        #有一些二手房信息缺少部分信息，如：缺少装修信息，或者缺少楼层信息，这时候需要加个判断，不然爬取就会中断。\n",
    "        if div[\\\n",
    "        #'Name', 'Type', 'Area', 'Towards', 'Floor', 'Decorate', \\\n",
    "        'Address'\\\n",
    "        #, 'TotalPrice', 'Price'\\\n",
    "        ] == None:\n",
    "                return None\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "def main():\n",
    "    for i in range(1,3):\n",
    "        url = 'https://dl.58.com/tech/pn{}/'.format(i)\n",
    "        content = get_one_page(url)\n",
    "        print('第{}页抓取完毕'.format(i))\n",
    "        for div in parse_one_page(content):\n",
    "            print(div)\n",
    "        with open('Data2.csv', 'a', newline='') as f:  # Data.csv 文件存储的路径,如果默认路径就直接写文件名即可。\n",
    "            fieldnames = [\\\n",
    "            #'Name', 'Type', 'Area', 'Towards', 'Floor', 'Decorate', \\\n",
    "            'Address'\\\n",
    "            #, 'TotalPrice', 'Price'\\\n",
    "            ]\n",
    "            writer = csv.DictWriter(f, fieldnames=fieldnames)\n",
    "            writer.writeheader()\n",
    "            for item in parse_one_page(content):\n",
    "                writer.writerow(item)\n",
    "        time.sleep(3)#设置爬取频率，一开始我就是爬取的太猛，导致网页需要验证。\n",
    "\n",
    "if __name__=='__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
